# Parir

Parir is a Python library performing GPU acceleration of Python functions
containing for-loops based on simple annotations provided by the user when
calling an annotated function, with a high degree of control and
customizability.

## Installation

The dependencies of Parir can either be installed manually, or you can use the
provided `Dockerfile` which installs Parir and includes all dependencies needed
to run tests and benchmarks. Below, we refer to the main dependencies and where
to find installation instructions for them.

Once the external dependencies have been installed, you can install Parir along
with its Python dependencies using
```
pip install .
```

### Manual install

The Parir compiler is implemented in [Rust](https://www.rust-lang.org/), so it
depends on the Rust compiler `rustc` to be available in the path. Parir also
requires [Python](https://www.python.org/) and `pip` to be installed.

To be able to run the code generated by the Parir compiler, you need to install
[CUDA](https://developer.nvidia.com/cuda-downloads) and ensure that `nvcc` (the
CUDA compiler) is in the `PATH`.

## Example

Assume we have defined a function in Python for computing the row-wise sum of a
two-dimensional Torch tensor `x` and storing the result in `out`:
```
def sum_rows(x, out, N, M):
  for i in range(N):
    out[i] = 0.0
    for j in range(M):
      out[i] = out[i] + x[i,j]
```

Note that the iterations of the outer for-loop over `i` are independent.
Therefore, we can parallelize this function using Parir. Paralellization using
Parir is performed in two steps. First, we annotate the function to indicate
that we want to parallelize it:
```
import parir

@parir.jit
def sum_rows(x, out, N, M):
  ... (as before)
```

When the Python interpreter reaches this function, the Parir compiler will
attempt to translate the Python code to an untyped, Python-like representation.
The second step occurs when we call the function. At this point, the compiler
will add types to its representation based on the provided arguments. If the
function is called like
```
sum_rows(x, out, N, M)
```

it will be executed by the Python interpreter. We need to provide an additional
argument `parallelize` to specify how to parallelize the for-loops of the
function. When we perform parallelization, the user must ensure the
tensor arguments are allocated on the GPU. Assuming `x` and `out` are on the
GPU, we can adjust the call to parallelize the outer for-loop as
```
sum_rows(x, out, N, M, parallelize={'i': [Threads(N)]})
```

where we indicate that each iteration of the `i` loop should run in parallel.
To further improve performance, we note that the inner loop over `j` performs a
summation which we can also parallelize. However, in this case we also need to
indicate that we are performing a [reduction](https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29)
or the compiler will generate incorrect code. The resulting call becomes
```
p = {'i': [Threads(N)], 'j': [Threads(128), Reduction()]}
sum_rows(x, out, N, M, parallelize=p)
```

See the `test` directory for complete examples using Parir. The example
discussed above is based on `test/test_sum.py`.

## Argument types

The main way to control the types of arguments is via the `dtype` assigned to
the Torch tensor. A tensor containing a single value, with empty shape, is
interpreted as a scalar value with the specified type. An integer or float from
Python is interpreted as a 64-bit integer or float, respectively. Parir exposes
functions for converting the type of an expression, when more granular control
is required. For instance, `parir.float32(e)` converts the type of the
expression `e` to `float32`.

Parir also support dictionary arguments, when all keys are strings and each of
these are mapped to valid (non-dictionary) values. We can use this to reduce
the number of arguments passed and potentially also to reuse argument
structures across multiple functions. The `test/test_record_args.py` file
contains a simple example use of a record, while `test/test_forward.py` also
shows how records can be reused across multiple functions.

## Parallelization

The approach used to map parallelism to CUDA is fairly straightforward. The
innermost level of parallelism is mapped to CUDA threads, while any
parallelized outer for-loops are mapped to CUDA blocks. Importantly, we require
that all statements on the same level of nesting in a parallel for-loop have
the same amount of parallelism (or are sequential). This means that, we cannot
have subsequent nested for-loops, such as
```
for i in range(N):
  for j1 in range(M1):
    ...
  for j2 in range(M2):
    ...
```

where the `j1` loop uses 512 threads and the `j2` loop uses 256 threads. In
this case, half the threads would be idle when running the second loop, which
is wasteful performance-wise. If we want to use a different number of threads
in the two loops, we can rewrite the code as
```
for i in range(N):
  for j1 in range(M1):
    ...
for i in range(N):
  for j2 in range(M2):
    ...
```

This results in two separate kernels where half the threads do not need to be
idle in the `j2` loop.

If we have a function where multiple for-loops iterate over a variable with the
same name (say, `i`), any properties specified on `i` apply to all loops using
a variable `i`. We can use this to reduce the number of entries in the
parallelization dictionary. However, a drawback is that we may need to rename
loop variables when we need two loops to parallelize differently (e.g., if one
of them performs a reduction). This design is intentional, as it avoids the
need to add the parallel annotations inline in the function, which would lock
us in to use a particular parallelization strategy.

## Working around limitations

Many parts of a typical Python program is inherently sequential or includes
code that is difficult to parallelize efficiently. For this reason, Parir
(intentionally) does not support many features of Python. Parir is designed to
be used in small functions where we have lots of potential parallelism and the
performance is critical. As Parir operates on Torch tensors, we can also rely
on the Torch library functions when these are available.

There are many low-level concepts that are critical to achieve
high-performance, but that are not immediately accessible from Parir, such as
[tensor cores](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)
and [shared memory](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/))
in CUDA. In such situations, we can retrieve the generated code from Parir,
manually modify the code, and pass the updated code to Parir to produce a
function useable from Python. The Parir API exposes two functions for this
purpose.

The `print_compiled` expects three arguments
1. The function that is being compiled
2. A list of the arguments that should be passed to the function
3. The parallelization dictionary

and it compiles the provided function based on the arguments to CUDA C++ code,
and returns the resulting code as a string. An advanced user can store the
output from this function in a file and modify the file to use unsupported
features of CUDA C++. The `compile_string` function generates a Python wrapper
calling the low-level CUDA code, given the name of the function and the CUDA
C++ code (both provided as strings).

When modifying the generated code, it is critical that the exposed C API of
functions marked with `extern "C"` is not changed, or the Python wrapper will
make invalid calls to the resulting shared library.
