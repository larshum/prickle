# Parir

Parir is a Python library performing GPU acceleration of Python functions
containing for-loops based on simple annotations provided by the user when
calling an annotated function, with a high degree of control and
customizability.

## Installation

The dependencies of Parir can either be installed manually, or you can use the
provided `Dockerfile` which installs Parir and includes all dependencies needed
to run tests and benchmarks. Below, we refer to the main dependencies and where
to find installation instructions for them.

Once the external dependencies have been installed, you can install Parir along
with its Python dependencies using
```
pip install .
```

### Manual Install

The Parir compiler is implemented in [Rust](https://www.rust-lang.org/), so it
depends on the Rust compiler `rustc` to be available in the path. Parir also
requires [Python](https://www.python.org/) and `pip` to be installed.

To be able to run the code generated by the Parir compiler, you need to install
[CUDA](https://developer.nvidia.com/cuda-downloads) and ensure that `nvcc` (the
CUDA compiler) is in the `PATH`.

## Example

Assume we have defined a function in Python for computing the row-wise sum of a
two-dimensional Torch tensor `x` and storing the result in `out`:
```python
def sum_rows(x, out, N, M):
  for i in range(N):
    for j in range(M):
      out[i] += x[i,j]
```

Note that the iterations of the outer for-loop over `i` are independent.
Therefore, we can parallelize this function using Parir. Paralellization using
Parir is performed in two steps. First, we annotate the function and label the
statements we want to parallelize:
```python
import parir

@parir.jit
def sum_rows(x, out, N, M):
  parir.label('i')
  for i in range(N):
     parir.label('j')
     for j in range(M):
       out[i] += x[i,j]
```

When the Python interpreter reaches this function, the Parir compiler will
attempt to translate the Python code to an untyped, Python-like representation.
The second step occurs when we call the function. At this point, the compiler
will add types to its representation based on the provided arguments. We need
to provide an extra keyword argument `parallelize` to specify how the labeled
statements should be parallelized. Assume we want to parallelize the outer
loop, and have the inner loop execute as a parallel [reduction](https://en.wikipedia.org/wiki/Fold_%28higher-order_function%29).
In this case, we could call the function as (assuming `x` and `out` are
allocated on the GPU) follows, where we parallelize by referring to the labels
in the function as
```
p = {'i': [parir.threads(32)], 'j': [parir.threads(128), parir.reduce()]}
sum_rows(x, out, N, M, parallelize=p)
```

In this case, the outer loop will execute across 32 CUDA blocks, while the
reduction of the inner loop will run among 128 CUDA threads. We discuss the
approach to mapping parallelism to CUDA threads and blocks later.

If we, for debugging reasons, want to run the code in the Python interpreter,
we can set the `seq` keyword argument to `True` when calling the function,
without having to remove the annotation on the function.

See the `test` directory for complete examples using Parir. The example
discussed above is based on `test/test_sum.py`.

## Types

The main way to control the types of arguments is via the `dtype` assigned to
the Torch tensor. A tensor containing a single value, with empty shape, is
interpreted as a scalar value with the specified type. An integer or float from
Python is interpreted as a 64-bit integer or float, respectively. Parir exposes
functions for converting the type of an expression, when more granular control
is required. For instance, `parir.float32(e)` converts the type of the
expression `e` to `float32`.

Parir also support dictionary arguments, when all keys are strings and each of
these are mapped to valid (non-dictionary) values. We can use this to reduce
the number of arguments passed and potentially also to reuse argument
structures across multiple functions. The `test/test_record_args.py` file
contains a simple example use of a record, while `test/test_forward.py` also
shows how records can be reused across multiple functions.

## Argument Specialization

Parir will automatically specialize the compilation based on the provided
arguments, by inlining the values of all scalar parameter values. For instance,
if we call a decorated function `f` as
```python
f(t, 10)
```

any uses of the second argument in `f` will be replaced by the literal `10`.
A Torch scalar `torch.tensor(10, dtype=torch.int16)` is considered equivalent
to a Python integer literal, except that it allows us to explicitly specify the
type of the integer. Python literals are treated as 64-bit values. For Torch
tensors with one or more dimensions, the shape of the tensor is used for
specialization.


The overhead of JIT compilation can be significant when called in a loop as
```python
for i in range(N):
  f(t, i)
```

because the updated value of `i` triggers a JIT compilation in each iteration.
To work around this, we can wrap the value in a tensor as
```python
for i in range(N):
  ix = torch.tensor([i], dtype=torch.int64)
  f(t, ix)
```

and update all uses of `i` with `i[0]` in the definition of `f`.

## Parallelization

The approach used to map parallelism to CUDA is fairly straightforward. The
innermost level of parallelism is mapped to CUDA threads, while any
parallelized outer for-loops are mapped to CUDA blocks. Importantly, we require
that all statements on the same level of nesting in a parallel for-loop have
the same amount of parallelism (or are sequential). This means that we cannot
have subsequent nested for-loops, such as
```python
for i in range(N):
  for j1 in range(M1):
    ...
  for j2 in range(M2):
    ...
```

where the `j1` loop uses 512 threads and the `j2` loop uses 256 threads. In
this case, half the threads would be idle when running the second loop, which
is wasteful performance-wise. If we want to use a different number of threads
in the two loops, we can rewrite the code as
```python
for i in range(N):
  for j1 in range(M1):
    ...
for i in range(N):
  for j2 in range(M2):
    ...
```

This results in two separate kernels where half the threads do not need to be
idle in the `j2` loop, which is supported in Parir.

### Sequential Code

As running sequential code on the GPU is significantly slower than on the CPU,
any sequential code (outside parallel loops) in a decorated function runs on
the CPU. All parameter data is allocated on the GPU, so an assignment in
sequential code (which runs on the CPU) may require expensive copying of data
and book-keeping to make sure data on the CPU and the GPU remains consistent.
For this reason, assignments outside parallel code (which run on the CPU) are
not allowed.

However, in certain situations, we may be willing to pay this extra cost.
Consider, for instance, the below function for computing the sum. Even if we
parallelize the reduction loop, the assignment to `out[0]` ends up outside the
parallel code, which results in an error.
```python
import parir
@parir.jit
def sum(x, out, N):
  out[0] = 0.0
  parir.label('i')
  for i in range(N):
    out[0] += x[i]
```

To work around this, we can use the `parir.gpu` context. All code within this
context runs on the GPU. The fixed code looks like
```python
import parir
@parir.jit
def sum(x, out, N):
  with parir.jit:
    out[0] = 0.0
    parir.label('i')
    for i in range(N):
      out[0] += x[i]
```

Under the hood, this introduces a for-loop with a single iteration, which is
marked as running in parallel using `1` thread.

## Working Around Limitations

Many parts of a typical Python program is inherently sequential or includes
code that is difficult to parallelize efficiently. For this reason, Parir
(intentionally) does not support many features of Python. Parir is designed to
be used in small functions where we have lots of potential parallelism and the
performance is critical. As Parir operates on Torch tensors, we can also rely
on the Torch library functions when these are available.

There are many low-level concepts that are critical to achieve
high-performance, but that are not immediately accessible from Parir, such as
[tensor cores](https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/)
and [shared memory](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/))
in CUDA. In such situations, we can retrieve the generated code from Parir,
manually modify the code, and pass the updated code to Parir to produce a
function useable from Python. The Parir API exposes two functions for this
purpose.

For instance, consider the `sum_rows` function presented as an example above.
To print the resulting CUDA code from compiling this function, we can use
```python
p = {'i': [parir.threads(32)], 'j': [parir.threads(128), parir.reduce()]}
print(parir.print_compiled(sum_rows, [x, out, N, M], p))
```

We pass a function to be compiled (this function does _not_ have to be
decorated with `@parir.jit`), a list of the arguments to be passed to the
function, and the parallelization dictionary (which we would pass to the
`parallelize` keyword argument in a regular function call). The resulting
string is printed to stdout using `print`.

An advanced user could store the output code to a file and modify it manually.
This is useful for performing operations not supported by the Parir compiler
(as discussed above), or to debug the generated code. Assuming the modified
code of the function `sum_rows` is loaded to a string `s`, we compile it as
```python
fn = parir.compile_string("sum_rows", s, includes=[], libs=[], extra_flags=[])
```

where the resulting function `fn` is a callable Python function expecting the
same arguments as the original `sum_rows` function. The two required arguments
is the name of the function and the CUDA C++ code. The `includes` and `libs`
keyword arguments specify include paths (`-I`) and library paths (`-L`) to be
passed to the `nvcc` compiler (e.g., if the modified code depends on external
libraries), and the `extra_flags` keyword argument provides custom arguments to
be passed to `nvcc`.
